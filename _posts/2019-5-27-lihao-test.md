---
layout: post
title: "决策树算法熵问题"
subtitle: "After the release of Wechat Mini-Program"
author: "jiutian"
header-img: "img/post-bg-web.jpg"
header-mask: 0.3
tags:
  - 算法
  - 机器学习
---

> 李浩

### 简介
决策树是一种十分常用的监督学习的分类算法。所谓监管学习，就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。
### 算法分类
决策树算法目前主要有以下三种：ID3/C4.5/CART
- ID3算法使用的是信息熵增益
- C4.5算法使用的是信息熵增益率
- CART算法使用的是Gini系数

### 优缺点
- 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关的特征数据。

- 缺点：可能会产生过度匹配问题。

- 适用数据类型：数值型和标称型

### 信息量

首先是信息量。假设我们听到了两件事,分别如下：

事件A：巴西队进入了2018世界杯决赛。

事件B：中国队进入了2018世界杯决赛。

仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。因此信息量可以定义如下
$$
I(X=x_{i})=-log_{2}p(x_{i})
$$

#### 信息熵
信息熵便是信息的期望值，可以记作：
$$
H(X)=\sum_{i=1}^np(x_i)I(x_{i})=-\sum_{i=1}^np(x_i)log_{2}p(x_{i})
$$
熵：表示随机变量的不确定性。变量不确定性越高，熵越高。
#### 条件熵
X给定条件下Y的条件分布的熵对X的数学期望，在机器学习中为选定某个特征后的熵，公式如下：
$$
H(Y|X)=\sum_{x}p(x)H(Y|X=x)
$$

#### 信息增益
信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下：
$$
IG(Y|X)=H(Y)-H(Y|X)
$$

#### 信息增益比
特征的信息增益熵与该特征的信息熵的比值。
$$
g_r=\frac{IG(Y|X)}{splitEntropy(x)}
\\
其中
\\
splitEntropy(X)=-\sum_{i=1}^n(\frac{|X_i|}{|X|})log_2(\frac{X_i}{X})
$$
选取最大的信息增益率作为分裂属性。

#### Gini系数
Gini系数是一种与信息熵类似的做特征选择的方式，可以用来数据的不纯度。在CART算法中,基尼不纯度表示一个随机选中的样本在子集中被分错的可能性。基尼不纯度为这个样本被选中的概率乘以它被分错的概率。当一个节点中所有样本都是一个类时，基尼不纯度为零。
假设y的可能取值为{1, 2, ..., m},令fi是样本被赋予i的概率，则基尼指数可以通过如下计算：
$$
Gini(p)=\sum_{i=1}^np_k(1-p_i)=1-\sum_{i=1}^np_i^2
$$
CART算法中的基尼指数:在CART(Classification and Regression Tree)算法中利用基尼指数构造二叉决策树。 

Gini系数的计算方式如下： 
$$
Gini(D)=1-\sum_{i=1}^np_i^2
$$
其中，D表示数据集全体样本，pi表示每种类别出现的概率。

取个极端情况，如果数据集中所有的样本都为同一类，那么有p0=1，Gini(D)=0，显然此时数据的不纯度最低，**即选择Gini系数较小的作为分类特征**。

与信息增益类似，我们可以计算基尼系数增益如下表达式：
$$
{\Delta}Gini(X)=Gini(D)-Gini_{X}(D)
$$
上面式子表述的意思就是，加入特征X以后，数据不纯度减小的程度。在做特征选择的时候，**我们可以取ΔGini(X)最大的那个**。
#### [交叉熵/相对熵](https://blog.csdn.net/xbmatrix/article/details/56691137)
[参考一](https://www.cnblogs.com/skyfsm/p/6790245.html)

### 树剪枝

即在构建树叉时，由于数据中的噪声和离群点，许多分支反映的是训练数据中的异常，而树剪枝则是处理这种过分拟合的数据问题，常用的剪枝方法为先剪枝和后剪枝。后文详细描述。

###### 习题一
1.决策树算法example one

ID | sex(A)| car type(B)| 衬衣size(C)| class
---|---|---|---|---
1 | male|家用|小|A
2 |male |运动|中|A
 3|male |运动|中|A
4 |male |运动|大|A
5 |male |运动|加大|A
6 |male |运动|加大|A
 7|female|运动|小|A
 8|female|运动|小|A
 9|female|运动|中|A
 10|female|豪华|大|A
 11|male |家用|大|B
 12|male |家用|加大|B
 13|male |家用|中|B 
 14|male |豪华|加大|B
 15|female |豪华|小|B
 16|female |豪华|小|B
 17|female |豪华|中|B
 18|female |豪华|中|B
19 |female |豪华|中|B
 20|female |豪华|大|B
 **问题**

1.计算信息熵及信息熵增益？

2.计算Gini系数？

a. 信息熵与信息增益：
$$
H(X)=-\sum_{i=1}^np(x_i)log_{2}p(x_{i})
$$

- 划分前样本集的总信息熵：
$$
E=-0.5*log_20.5-0.5*log_20.5=1
$$

1. 按照特性`sex`的信息熵：
$$
E_{sex=female}=-\frac{4}{10}*log_2(\frac{4}{10})-\frac{6}{10}*log_2(\frac{6}{10})=0.971\\
E_{sex=male}=-\frac{4}{10}*log_2(\frac{4}{10})-\frac{6}{10}*log_2(\frac{6}{10})=0.971
$$

则按照`sex`属性划分样本集的信息增益为：
$$
{\Delta}_{female}=E-\frac{10}{20}*E_{sex=female}-\frac{10}{20}*E_{sex=male}=0.029
$$

2. 按照特征`car type`的信息熵为；
$$
E_{type=家用}=-\frac{1}{4}*log_2(\frac{1}{4})-\frac{3}{4}*log_2(\frac{3}{4})=0.811
\\E_{type=运动}=-\frac{8}{8}*log_2(\frac{8}{8})-\frac{0}{8}*log_2(\frac{0}{8})=0
\\E_{type=豪华}=-\frac{0}{8}*log_2(\frac{0}{8})-\frac{8}{8}*log_2(\frac{8}{8})=0
$$

则按照特征`car type`属性划分则样本信息增益为：
$$
{\Delta}_{car type}=E-\frac{4}{20}*E_{type=家用}-\frac{8}{20}*E_{type=运动}-\frac{8}{20}*E_{type=豪华}=0.8378
$$

3. 按照特征`size`划分的信息熵为：
$$
E_{szie=小}=-\frac{3}{5}*log_2(\frac{3}{5})-\frac{2}{5}*log_2(\frac{2}{5})=0.971\\
E_{szie=中}=-\frac{3}{7}*log_2(\frac{3}{7})-\frac{4}{7}*log_2(\frac{4}{7})=0.9852\\
E_{szie=大}=-\frac{2}{4}*log_2(\frac{2}{4})-\frac{2}{4}*log_2(\frac{2}{4})=1\\
E_{szie=加大}=-\frac{2}{4}*log_2(\frac{2}{4})-\frac{2}{4}*log_2(\frac{2}{4})=1\\
$$

则按照特征`car type`属性划分则样本的信息增益为：
$$
{\Delta}_{size}=E-\frac{5}{20}*E_{size=小}-\frac{7}{20}*E_{size=中}-\frac{4}{20}*E_{size=大}-\frac{4}{20}*E_{size=加大}=0.01243
$$
则根据信息熵增益，则选特性`car type`作为分割第一特征。

2. 信息增益比
  

信息增益比为该特征信息增益比与该特征信息熵的比，具体计算公式如上文理论中。此处及下文中都是针对其中一个特征为例说明问题，此处选择具有代表性的特征C说明。

特征C的信息熵增益在上面已经计算出：0.01243

那么特征A的信息熵为：
$$
splitEntropy(A)=-\sum_{i=1}^n(\frac{|X_i|}{|X|})log_2(\frac{X_i}{X})
\\=-\frac{5}{20}*log_2(\frac{5}{20})-\frac{7}{20}*log_2(\frac{7}{20})-\frac{4}{20}*log_2(\frac{4}{20})-\frac{4}{20}*log_2(\frac{4}{20})=0.959
$$
那么特征C的信息增益比为：0.01243/0.959=0.0129。同理可计算其他几个特征的信息增益比。

3. 根据计算 GINI 公式：
$$
Gini(D)=1-\sum_{i=1}^np_i^2
$$



```bash
	1. 整体Gini值：1-(1/2)^2-(1/2)^2 =0.5
	2. ID 每个都不一样，与其他人没有共性，所以GINI=0
	3. 性别 ：1-(1/2)^2-(1/2)^2 =0.5 
	4. 家用： 1-(1/4)2-(3/4)2 = 0.375  
	   运动： 1-(0/8)2-(8/8)2 = 0 
	   豪华： 1-(1/8)2-(7/8)2 = 0.218
	   车型GINI=4/20*0.375+8/20*0.218 = 0.16252
```
多路划分属性统计表：

<html>

  <table>
<tr>
    <td>class</td>
    <th colspan="4">衣服种类</th>
</tr>
<tr>
    <td></td>
    <td>小</td>
    <td>中</td>
    <td>大</td>
    <td>加大</td>
</tr>
<tr>
    <td>A</td>
    <td>3</td>
    <td>3</td>
    <td>2</td>
    <td>2</td>
</tr>
<tr>
    <td>B</td>
    <td>2</td>
    <td>4</td>
    <td>2</td>
    <td>2</td>
</tr>
</table>
</html>
<html>
  <table>
<tr>
    <td>class</td>
    <th colspan="3">车型 </th>
</tr>
<tr>
    <td></td>
    <td>家用</td>
    <td>运行</td>
    <td>豪华</td>
</tr>
<tr>
    <td>A</td>
    <td>1</td>
    <td>8</td>
    <td>1</td>
</tr>
<tr>
    <td>B</td>
    <td>3</td>
    <td>0</td>
    <td>7</td>
</tr>
</table>
</html>

```shell
5. 三种尺码GINI系数：
   小：1-(3/5)2-(2/5)2 = 0.48
   中：1-(3/7)2-(4/7)2 = 0.4898 
   大：1-(2/4)2-(2/4)2 = 0.5 
   加大：1-(2/4)2-(2/4)2 = 0.5 
   
   衬衣GINI：5/20*0.48+7/20*0.4898+4/20*0.5+4/20*0.5 = 0.4914 
    
6.	属性比较：通过上述计算，显然车型不纯度高，更容易划分
```
###### 习题二 二分类问题数据集

A | B|类标号  
---|---|---
T|F|+
T|T|+
T|T|+
T|F|-
T|T|+
F|F|-
F|F|-
F|F|-
T|T|-
T|F|-
<html>
  <table>
<tr>
    <th colspan="3">统计A</th>
     <th colspan="3">统计B</th>
</tr>
<tr>
    <td></td>
    <td>A=T</td>
    <td>A=F</td>
    <td>B=T</td>
    <td>B=F</td>
</tr>
<tr>
    <td>+</td>
    <td>4</td>
     <td>0</td>
     <td>3</td>
     <td>1</td>
</tr>
<tr>
    <td>-</td>
    <td>3</td>
     <td>3</td>
     <td>1</td>
     <td>5</td>
</tr>
</table>
</html>

##### 信息增益计算

1. 计算按照属性A和B划分时的信息增益。决策树归纳算法将会选择那个属性？

2. 计算按照属性A和B划分时GINI指标。决策树归纳算法将会选择那个属性？

3. 熵和GINI指标在区间 [0,0.5] 都是单调递增，在区间 [0,0.5] 单调递减。有没有可能信息增益和GINI指标增益支持不同的属性？解释你的理由。

- 信息熵：
$$
H(X)=-\sum_{i=1}^np(x_i)log_{2}p(x_{i})
$$

1. 划分前样本集的总信息熵：

$$
E=-0.4*log_20.4-0.6*log_20.6=0.971
$$

特征A的信息熵为：
$$
E_{A=T}=-\frac{4}{7}*log_2(\frac{4}{7})-\frac{3}{7}*log_2(\frac{3}{7})=0.9852\\
E_{A=F}=-\frac{0}{3}*log_2(\frac{0}{3})-\frac{3}{3}*log_2(\frac{3}{3})=0
$$
则按照A属性划分样本集的信息熵增益为：
$$
{\Delta}_A=E-\frac{7}{10}*E_{A=T}-\frac{3}{10}*E_{A=F}=0.2813
$$
同理B可得：
$$
{\Delta}_B=E-\frac{4}{10}*E_{B=T}-\frac{6}{10}*E_{B=F}=0.2565
$$
因此决策树归纳算法选A属性.
2. 按照属性A 、B划分样本集:

A指标：
$$
Gini=1-(\frac{4}{10})^2-(\frac{6}{10})^2=0.48\\
Gini_{A=T}=1-(\frac{4}{7})^2-(\frac{3}{7})^2=0.4898\\
Gini_{A=F}=1-(\frac{0}{3})^2-(\frac{3}{3})^2=0
$$
则Gini增益为：
$$
E_A=Gini-\frac{7}{10}*Gini_{A=T}-\frac{3}{10}*Gini_{A=F}=0.1371
$$
B指标：
$$
Gini_{B=T}=1-(\frac{3}{4})^2-(\frac{1}{4})=0.375\\
Gini_{B=F}=1-(\frac{1}{6})^2-(\frac{5}{6})^2=0.2778
$$
则Gini增益为：
$$
E_B=Gini-\frac{4}{10}*Gini_{B=T}-\frac{6}{10}*Gini_{B=F}=0.1633
$$
因此决策树算法选择B;

3.信息增益考察的是特征对整个数据贡献，没有到具体的类别上，所以一般只能用来做全局的特征选择
Gini系数是一种与信息熵类似的做特征选择的方式，用来数据的不纯度。在做特征选择的时候，我们可以取ΔGini(X)最大的那个。


###### 习题三：满意度数据描述
&emsp;利用满意度调查数据来描述决策树算法。假如天热气不能用了，相关部门维修后，需要对这次修理障碍过程进行回访，然后给出相应评价，满意或者不满意。根据历史数据可以建立满意度预警模型，建模的目的：预测哪些用户会给出不满意的评价。目标变量为二分类变量：满意（记为0）和不满意（记为1）。自变量为障碍类型、障碍原因、修障总时长、最近一个月发生故障的次数、最近一个月不满意次数。简单的数据如下：

客户ID | 故障原因(A) |故障类型(B) |修障时长(C)|满意度
---|---|--|--|--|--
001 | 1|5|10.2|1
002 | 1|5|12|0
003 | 1|5|14|1
004 | 2|5|16|0
005 | 2|5|18|1
006 | 2|6|20|0
007 | 3|6|22|1
008 | 3|6|23|0
009 | 3|6|24|1
010 | 3|6|25|0

其中故障原因和故障类型为离散型变量，分别为原因ID和类型ID。修障时长为连续型变量，单位为小时。满意度中1为不满意、0为满意。

接下来沿着分类特征的选择和树剪枝两条主线，去描述三种决策树算法构造满意度预警模型：
分类特征选择：即该选择故障原因、故障类型、修障时长三个变量中的哪个作为决策树的第一个分类特征。
ID3算法是采用信息熵增益来选择树叉，c4.5算法采用信息熵增益率，CART算法采用Gini指标。此外离散型变量和连续型变量在计算信息增益、增益率、Gini指标时会有些区别。详细描述如下：
1.ID3算法的信息熵增益：
信息增益的思想来源于信息论的香农定理，ID3算法选择具有最高信息增益的自变量作为当前的树叉（树的分支），以满意度预警模型为例，模型有三个自变量：故障原因、故障类型、修障时长。分别计算三个自变量的信息增益，选取其中最大的信息增益作为树叉。信息增益=原信息需求-要按某个自变量划分所需要的信息。

- 信息熵公式为：
$$
H(X)=-\sum_{i=1}^np(x_i)log_{2}p(x_{i})
$$

1. 划分前样本总信息熵：
$$
E=-0.5*log_20.5-0.5*log_20.5=1
$$

a. 按特征故障原因(A)的信息熵为：
$$
E_{A=1}=-\frac{2}{3}log_{2}(\frac{2}{3})-\frac{1}{3}log_{2}(\frac{1}{3})=0.9182\\
E_{A=2}=-\frac{2}{3}log_{2}(\frac{2}{3})-\frac{1}{3}log_{2}(\frac{1}{3})=0.9182\\
E_{A=3}=-\frac{2}{4}log_{2}(\frac{2}{4})-\frac{2}{4}log_{2}(\frac{2}{4})=1\\
$$
则特征A的信息熵增益为：
$$
{\Delta}_A=E-\frac{6}{10}*E_{A=1}-\frac{4}{10}*E_{A=3}=0.0491
$$
b. 按特征故障类型(B)的信息熵为：
$$
E_{B=5}=-\frac{3}{5}log_{2}(\frac{3}{5})-\frac{2}{5}log_{2}(\frac{2}{5})=0.971\\
E_{B=6}=-\frac{3}{5}log_{2}(\frac{3}{5})-\frac{2}{5}log_{2}(\frac{2}{5})=0.971
$$
则特征B的信息熵增益为：
$$
{\Delta}_B=E-\frac{5}{10}*E_{B=5}-\frac{5}{10}*E_{B=6}=0.029
$$
c.按特征修障时长(C)d的信息熵为：
故障原因和故障类型两个变量都是离散型变量，按上述方式即可求得信息增益，但修障时长为连续型变量，对于连续型变量该怎样计算信息增益呢？只需将连续型变量由小到大递增排序，取相邻两个值的中点作为分裂点，然后按照离散型变量计算信息增益的方法计算信息增益，取其中最大的信息增益作为最终的分裂点。如求修障时长的信息增益，首先将修障时长递增排序，即10.2、12、14、16、18、20、22、23、24、25,取相邻两个值的中点，如10.2和12，中点即为（10.2+12）/2=11.1,同理可得其他中点，分别为11.1、13、15、17、19、21、22.5、23.5、24.5。对每个中点都离散化成两个子集，如中点11.1，可以离散化为两个<=11.1和>11.1两个子集，然后按照离散型变量的信息增益计算方式计算其信息增益，如中点11.1的信息增益计算过程如下：
$$
E_{C<11.1}=-\frac{1}{1}log_{2}(\frac{1}{1})=0\\
E_{C>11.1}=-\frac{4}{9}*log_{2}(\frac{4}{9})-\frac{5}{9}*log_{2}(\frac{5}{9})=0.991
$$
则对应的信息熵为：
$$
{\Delta}_{11.1}=E-\frac{1}{10}*E_{C<11.1}-\frac{9}{10}*E_{C>11.1}=0.0089
$$
同理分别求得各个中点的信息增益，选取其中最大的信息增益作为分裂点，如取中点11.1。然后与故障原因和故障类型的信息增益相比较，取最大的信息增益作为第一个树叉的分支，此例中选取了故障原因作为第一个分叉。按照同样的方式继续构造树的分支。

2.C4.5算法增益率：
由于信息增益选择分裂属性的方式会倾向于选择具有大量值的属性（即自变量），如对于客户ID，每个客户ID对应一个满意度，即按此变量划分每个划分都是纯的（即完全的划分，只有属于一个类别），客户ID的信息增益为最大值1。但这种按该自变量的每个值进行分类的方式是没有任何意义的。为了克服这一弊端，有人提出了采用增益率（GainRate）来选择分裂属性。计算方式如下：
$$
g_r=\frac{IG(Y|X)}{splitEntropy(x)}\\
其中\\
splitEntropy(X)=-\sum_{i=1}^n(\frac{|X_i|}{|X|})log_2(\frac{X_i}{X})
$$
以特征A举例说明。特征A的信息增益在前面已经算出来：0.0491

那么下面就只需酸楚splitEntropy(X)就可以了。
$$
splitEntropy(X)=-\frac{3}{10}*log_2(\frac{3}{10})-\frac{3}{10}*log_2(\frac{3}{10})-\frac{4}{10}*log_2(\frac{4}{10})=1.571
$$
则特征A的信息增益比为：0.049/1.571=0.031
## 应用：kaggle竞赛之Titanic



#### 参考
1. [决策树模型 ID3/C4.5/CART算法比较](http://www.cnblogs.com/wxquare/p/5379970.html)
2. [**决策树（一）**](http://leijun00.github.io/2014/09/decision-tree/)
3. [**python人工智能：完整的图片识别(非图片验证码)，以及模型的使用**](https://www.jianshu.com/p/5b4e51869e64)
4. [机器学习笔记十六之基尼系数、CART](https://www.devtalking.com/articles/machine-learning-16/)
5. [决策树基础](https://applenob.github.io/decision_tree.html)
6. [数据挖掘十大算法之决策树详解（1）](https://www.ctolib.com/topics-96320.html)
7. [决策树ID3、C4.5、CART算法：信息熵，区别，剪枝理论总结](https://blog.csdn.net/ljp812184246/article/details/47402639)
8. [决策树信息增益率解决信息增益bug](https://blog.csdn.net/Chen_Meng_/article/details/82083548)
9. [信息熵、条件熵、联合熵、互信息、相对熵、交叉熵](https://www.jianshu.com/p/2ea0406d0793)
10. [详解机器学习中的熵、条件熵、相对熵、交叉熵](https://yq.aliyun.com/articles/623626/) 
11. [一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉](https://mp.csdn.net/mdeditor/90417547#)
